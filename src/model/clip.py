from typing import Tuple
import torch
import torch.nn as nn
from PIL import Image
import open_clip


def freeze_partial_layers(model: nn.Module, trainable_ratio: float = 0.3):
    """
    –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ X% —Å–ª–æ—ë–≤ –º–æ–¥–µ–ª–∏.
    trainable_ratio=0.3 ‚Üí –æ–±—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30% –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
    """
    all_params = list(model.named_parameters())
    total = len(all_params)
    cutoff = int(total * (1 - trainable_ratio))
    for i, (_, p) in enumerate(all_params):
        p.requires_grad = i >= cutoff

    print(f"‚úÖ Fine-tuning {trainable_ratio * 100:.1f}% –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–ª–æ—ë–≤ "
          f"({total - cutoff}/{total})")


def create_clip_partial_classifier(
        num_classes: int = 1000,
        pretrained: bool = True,
        trainable_ratio: float = 0.3,
        model_name: str = "ViT-B-16",
        pretrained_dataset: str = "openai"
) -> Tuple[nn.Module, object]:
    """
    –°–æ–∑–¥–∞—ë—Ç CLIP –≤–∏–∑—É–∞–ª—å–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä –∏ —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–ª–æ—ë–≤.
    """
    model, _, preprocess = open_clip.create_model_and_transforms(
        model_name,
        pretrained=pretrained_dataset if pretrained else None
    )

    visual_encoder = model.visual
    in_features = visual_encoder.output_dim

    classifier = nn.Linear(in_features, num_classes)
    nn.init.trunc_normal_(classifier.weight, std=0.02)
    if classifier.bias is not None:
        nn.init.zeros_(classifier.bias)

    model = nn.Sequential(visual_encoder, classifier)

    freeze_partial_layers(model, trainable_ratio=trainable_ratio)
    return model, preprocess


if __name__ == "__main__":
    image_path = "../../data/mac-merged/0.png"
    image = Image.open(image_path).convert("RGB")

    # Fine-tune –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 30 % CLIP-—ç–Ω–∫–æ–¥–µ—Ä–∞
    model, preprocess = create_clip_partial_classifier(
        num_classes=2,
        pretrained=True,
        trainable_ratio=0.3,
        model_name="ViT-B-16",
        pretrained_dataset="openai"
    )

    model.eval()

    input_tensor = preprocess(image).unsqueeze(0)

    with torch.no_grad():
        output = model(input_tensor)

    probs = torch.nn.functional.softmax(output[0], dim=0)
    pred_class = probs.argmax().item()
    conf = probs[pred_class].item()

    print(f"üß† –ú–æ–¥–µ–ª—å: CLIP {model[0].__class__.__name__}")
    print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å: {pred_class}")
    print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {conf:.4f}")
    print(f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: {probs.tolist()}")
